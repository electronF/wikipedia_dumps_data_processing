{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extraction data step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "Ce programme n'utilise pas de parallélisation donc il risque de prendre un peu plus de temps\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import gzip\n",
    "import sys\n",
    "from tqdm import tqdm \n",
    "from collections import namedtuple\n",
    "\n",
    "FILEPROPS=namedtuple(\"Fileprops\", \"parser num_fields column_indexes\")\n",
    "\n",
    "#CATEGORYLINKS_PARSER=re.compile(r'(?P<row0>[0-9]+?),(?P<row1>\\'.*?\\'?),(?P<row2>\\'.*?\\'?),(?P<row3>\\'[0-9\\ \\-:]+\\'?),(?P<row4>\\'\\'?),(?P<row5>\\'.*?\\'?),(?P<row6>\\'.*?\\'?)')\n",
    "CATEGORYLINKS_PARSER=re.compile(r'^(?P<row0>[0-9]+?),(?P<row1>\\'.*?\\'?),(?P<row2>\\'.*?\\'?),(?P<row3>\\'[0-9\\ \\-:]+\\'?),(?P<row4>\\'.*?\\'?),(?P<row5>\\'[a-z\\-]*?\\'?),(?P<row6>\\'[a-z]+\\'?)$')\n",
    "PAGELINKS_PARSER=re.compile(r'^(?P<row0>[0-9]+?),(?P<row1>[0-9]+?),(?P<row2>\\'.*?\\'?),(?P<row3>[0-9]+?)$')\n",
    "LANGLINKS_PARSER=re.compile(r'^(?P<row0>[0-9]+?),(?P<row1>\\'.*?\\'?),(?P<row2>\\'.*?\\'?)$')\n",
    "REDIRECT_PARSER=re.compile(r'^(?P<row0>[0-9]+?),(?P<row1>-?[0-9]+?),(?P<row2>\\'.*?\\'?),(?P<row3>\\'.*?\\'?),(?P<row4>\\'.*?\\'?)$')\n",
    "CATEGORY_PARSER=re.compile(r'^(?P<row0>[0-9]+?),(?P<row1>\\'.*?\\'?),(?P<row2>[0-9]+?),(?P<row3>[0-9]+?),(?P<row4>[0-9]+?)$')\n",
    "PAGE_PROPS_PARSER=re.compile(r'^([0-9]+),(\\'.*?\\'),(\\'.*?\\'),(\\'[0-9\\ \\-:]+\\'),(\\'\\'),(\\'.*?\\'),(\\'.*?\\')$')\n",
    "PAGE_PARSER=re.compile((r'^(?P<row0>[0-9]+?),(?P<row1>[0-9]+?),(?P<row2>\\'.*?\\'?),(?P<row3>[0-9]+?),(?P<row4>[0-9]?),'\n",
    "    r'(?P<row5>[0-9\\.]+?),(?P<row6>\\'.*?\\'?),(?P<row7>(?P<row7val>\\'.*?\\'?)|(?P<row7null>NULL)),(?P<row8>[0-9]+?),(?P<row9>[0-9]+?),'\n",
    "    r'(?P<row10>(?P<row10val>\\'.*?\\'?)|(?P<row10null>NULL)),(?P<row11>(?P<row11val>\\'.*?\\'?)|(?P<row11null>NULL))$'))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# page\n",
    "`page_id` int(8) unsigned NOT NULL AUTO_INCREMENT,\n",
    "`page_namespace` int(11) NOT NULL DEFAULT 0,\n",
    "`page_title` varbinary(255) NOT NULL DEFAULT '',\n",
    "`page_is_redirect` tinyint(1) unsigned NOT NULL DEFAULT 0,\n",
    "`page_is_new` tinyint(1) unsigned NOT NULL DEFAULT 0,\n",
    "`page_random` double unsigned NOT NULL DEFAULT 0,\n",
    "`page_touched` varbinary(14) NOT NULL,\n",
    "`page_links_updated` varbinary(14) DEFAULT NULL,\n",
    "`page_latest` int(8) unsigned NOT NULL DEFAULT 0,\n",
    "`page_len` int(8) unsigned NOT NULL DEFAULT 0,\n",
    "`page_content_model` varbinary(32) DEFAULT NULL,\n",
    "`page_lang` varbinary(35) DEFAULT NULL,\n",
    "\n",
    "#langlinks\n",
    "`ll_from` int(8) unsigned NOT NULL DEFAULT 0,\n",
    "`ll_lang` varbinary(35) NOT NULL DEFAULT '',\n",
    "`ll_title` varbinary(255) NOT NULL DEFAULT '',\n",
    "\n",
    "# pagelinks\n",
    "`pl_from` int(8) unsigned NOT NULL DEFAULT '0',\n",
    "`pl_namespace` int(11) NOT NULL DEFAULT '0',\n",
    "`pl_title` varbinary(255) NOT NULL DEFAULT '',\n",
    "`pl_from_namespace` int(11) NOT NULL DEFAULT '0',\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "FILETYPE_PROPS=dict(\n",
    "        categorylinks=FILEPROPS(CATEGORYLINKS_PARSER, 7, (0, 1, 6)),\n",
    "        pagelinks=FILEPROPS(PAGELINKS_PARSER, 4, (0, 1, 2, 3)),\n",
    "        langlinks=FILEPROPS(LANGLINKS_PARSER, 3, (0, 1, 2)),\n",
    "        redirect=FILEPROPS(REDIRECT_PARSER, 5, (0, 1, 2)),\n",
    "        category=FILEPROPS(CATEGORY_PARSER, 5, (0, 1, 2, 3, 4)),\n",
    "        page_props=FILEPROPS(PAGE_PROPS_PARSER, 7, (0, 1)),\n",
    "        page=FILEPROPS(PAGE_PARSER, 12, (0, 1, 2, 3, 9, 10, 11)),\n",
    "        )\n",
    "\n",
    "#VALUE_PARSER=re.compile(r'\\(([0-9]+),(\\'.*?\\'),(\\'.*?\\'),(\\'[0-9\\ \\-:]+\\'),(\\'\\'),(\\'.*?\\'),(\\'.*?\\')\\)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_match(match, column_indexes):\n",
    "    row = match.groupdict()\n",
    "    return tuple(row[\"row{}\".format(i)] for i in column_indexes)\n",
    "\n",
    "\n",
    "def parse_value(value, parser, column_indexes, value_idx=0, pbar=None):\n",
    "    # replace unicode dash with ascii dash\n",
    "    value = value.replace(\"\\\\xe2\\\\x80\\\\x93\", \"-\")\n",
    "    parsed_correctly = False\n",
    "    for i, match in enumerate(parser.finditer(value)):\n",
    "        parsed_correctly = True\n",
    "        try:\n",
    "            row = parse_match(match, column_indexes)\n",
    "            yield row\n",
    "        except Exception as e:\n",
    "            print(\"Line: {!r}, Exception: {}\".format(value, e), file=sys.stderr)\n",
    "    if not parsed_correctly:\n",
    "        print(\"Line: {!r}, IDX: {}, Exception: {}\".format(value, value_idx, \"Unable to parse.\"), file=sys.stderr)\n",
    "\n",
    "\n",
    "def process_insert_values_line(line, parser, column_indexes, count_inserts=0, pbar=None):\n",
    "    start, partition, values = line.partition(' VALUES ')\n",
    "    # Each insert statement has format: \n",
    "    # INSERT INTO \"table_name\" VALUES (v1,v2,v3),(v1,v2,v3),(v1,v2,v3);\n",
    "    # When splitting by \"),(\" we need to only consider string from values[1:-2]\n",
    "    # This ignores the starting \"(\" and ending \");\"\n",
    "    values = values.strip()[1:-2].split(\"),(\")\n",
    "    pbar.set_postfix(found_values=len(values), insert_num=count_inserts)\n",
    "    for value_idx, value in enumerate(values):\n",
    "        for row in parse_value(value, parser, column_indexes, value_idx, pbar):\n",
    "            yield row\n",
    "\n",
    "\n",
    "def process_file(fp, fp_out, filetype, column_indexes=None, silent=False):\n",
    "    if filetype not in FILETYPE_PROPS:\n",
    "        raise Exception(\"Invalid filetype: {}\".format(filetype))\n",
    "    parser, num_fields, ci = FILETYPE_PROPS[filetype]\n",
    "    print(\"Parser: {}\\nnum_fields: {}\\nci: {}\".format(parser, num_fields, ci))\n",
    "    valid_row_keys = set([\"row{}\".format(i) for i in range(num_fields)])\n",
    "    if column_indexes is None:\n",
    "        column_indexes = ci\n",
    "    with tqdm(disable=silent) as pbar:\n",
    "        count_inserts = 0\n",
    "        for line_no, line in enumerate(fp, start=1):\n",
    "            if line.startswith('INSERT INTO `{}` VALUES '.format(filetype)):\n",
    "                count_inserts += 1\n",
    "                for row in process_insert_values_line(\n",
    "                        line, parser, column_indexes, count_inserts, pbar):\n",
    "                    if pbar is not None:\n",
    "                        pbar.update(1)\n",
    "                    print(\"\\t\".join(row), file=fp_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    files_to_open = {\n",
    "        \"page\":{\n",
    "            \"input\":\"ruwiki-latest-page.sql.gz\", \n",
    "            \"output\":\"pages_encoded_version.csv\"\n",
    "        }, \n",
    "        \"langlinks\":{\n",
    "            \"input\":\"ruwiki-latest-langlinks.sql.gz\", \n",
    "            \"output\":\"langlinks_encoded_version.csv\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    data_folder = 'data' \n",
    "    if not os.path.exists(data_folder):\n",
    "        os.makedirs(data_folder)\n",
    "        \n",
    "    for database_name in files_to_open:\n",
    "        with gzip.open(files_to_open[database_name][\"input\"], 'rt', encoding='ascii', errors='backslashreplace') as fp, open(os.path.join(data_folder, files_to_open[database_name][\"output\"]), 'wt', encoding='utf-8') as fp_out:\n",
    "            process_file(fp, fp_out, database_name, column_indexes=None, silent=None)\n",
    "    print(\"End of Processing\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. compute and save decoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "Note: Le multithreading est utilisé pour decomposer le travail et le repartir sur les differents procésseur.\n",
    "Cela contribu à l'optimisation de la vitesse de traitement des données. \n",
    "Et empeche de faire bugger l'ordinateur car au lieu de faire une grosse tache l'ordinateur fait plusieurs petite taches.\n",
    "\"\"\"\n",
    "\n",
    "#%%\n",
    "import os\n",
    "import threading\n",
    "from collections import defaultdict, namedtuple\n",
    "from typing import DefaultDict, List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_THREAD_START_NUMBER = 11\n",
    "MAX_BATCH = 1000\n",
    "INPUT_DATA_FOLDER = \"data\"\n",
    "OUTPUT_DATA_FOLDER = \"decoded\"\n",
    "LOCK = threading.RLock()\n",
    "\n",
    "build_url = lambda url: f\"https://ru.wikipedia.org/wiki/{url}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation du répertoire de sortie de données\n",
    "if not os.path.exists(OUTPUT_DATA_FOLDER):\n",
    "    os.makedirs(OUTPUT_DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_strings:DefaultDict[int, List[str]] = defaultdict(list)\n",
    "pages_langs = defaultdict(dict)\n",
    "pages_descriptions:DefaultDict[int, List[str]] = defaultdict(list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_lang_mapper(key:int)->(None):\n",
    "    \"\"\"\n",
    "    Creer un table de mappage clé-valeur. Cette table de mappage permet de lister les differentes langue dans\n",
    "    lesquels un article et traduits et son titre dans la version traduite.\n",
    "\n",
    "    Args:\n",
    "        key (int): The key of the given map who contains a MAX_BATCH line (for ex: 1000 lines of the input file).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    #Note la valeur est un dictionary dont les clés sont les codes iso correspondant à chaque langue et la valeur le titre de l'article\n",
    "    # dans la langue traduite\n",
    "\n",
    "    #Note: Cette approche permet de reduire la compléxité de fouille d'une langue de O(N) (où N est le nombre totale de langue existant \n",
    "    # sur wikipedia) à O(1). Cela permet de trouver si une traduction existe en 1 seule operation\n",
    "    for line in list_strings[key]:\n",
    "        with LOCK:\n",
    "            ll_from, ll_lang, ll_title = line.strip().split(\"\\t\")\n",
    "            ll_title = ll_title.strip().strip(\"'\").strip().encode(\"utf-8\").decode(\"utf8\").strip()\n",
    "            # Si le titre dans une langue correspond à une chaine de charactère vide alors la traduction dans cette langue n'existe pas\n",
    "            if ll_title != \"\":\n",
    "                pages_langs[int(ll_from)][ll_lang.strip(\"'\")] = ll_title\n",
    "\n",
    "    with LOCK:\n",
    "        #Libération de l'espace occupé en mémoire par les n lignes traitées.\n",
    "        list_strings[key].clear()\n",
    "        list_strings.pop(key)\n",
    "\n",
    "\n",
    "def find_right_fit_and_export(key:int, output_without_fr_en_file, output_without_fr_file):\n",
    "    \"\"\"\n",
    "        Cette procédure permet de trouver si une article ne possède pas la traduction francaise ou francaise et anglaise\n",
    "\n",
    "        Args:\n",
    "            key (int): The key of the given map who contains a MAX_BATCH line (for ex: 1000 lines of the input file).\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \n",
    "    \"\"\"\n",
    "    for line in pages_descriptions[key]:\n",
    "        with LOCK:\n",
    "            page_id, namespace, title, is_redirect = line.strip().split(\"\\t\")\n",
    "            if int(namespace) != 0 or int(is_redirect) != 0:\n",
    "                continue\n",
    "            \n",
    "            #Les titres étant des chaines de caractères sous forme de bytes array il faut effectuer decodage customizé\n",
    "            decoded_title = decode_text(b\"{0}\".format(title.strip().strip(\"'\")))\n",
    "            url = build_url(decoded_title)\n",
    "\n",
    "\n",
    "            #Tester si l'article courant n'a pas de traduction en francais et en anglais si oui l'Enregistrez dans un fichier\n",
    "            #sinon regarder s'il n'a pas de traduction en francais uniquement si oui l'enregistrer si non ne rien faire\n",
    "            langs_set = pages_langs.get(int(page_id))\n",
    "            if (langs_set == None) or (langs_set != None and (langs_set.get(\"en\") in (None, \"\")) and (langs_set.get(\"fr\") in (None, \"\"))):\n",
    "                    print(f\"{page_id}\\t\\t{decoded_title}\\t\\t{url}\", file=output_without_fr_en_file)\n",
    "            elif (langs_set != None and (langs_set.get(\"fr\") in (None, \"\"))):\n",
    "                    print(f\"{page_id}\\t\\t{decoded_title}\\t\\t{url}\", file=output_without_fr_file)\n",
    "    with LOCK:\n",
    "        #Libération de l'espace occupé en mémoire par les n lignes traitées.\n",
    "        pages_descriptions[key].clear()\n",
    "        pages_descriptions.pop(key)\n",
    "\n",
    "\n",
    "def decode_text(encoded_text:str) -> (str):\n",
    "        \"\"\"\n",
    "        Décodé le texte fournie en entrée et retourné le texte décodé en sortie.\n",
    "        \n",
    "        Args:\n",
    "            encoded_text: le texte encodé.\n",
    "        \n",
    "        Returns: \n",
    "            Le texte décodé.\n",
    "        \"\"\"\n",
    "    \n",
    "        ExtractedString = namedtuple(\"ExtractedString\", [\"string\", \"index\"])  \n",
    "\n",
    "        #Comme les texts encode peuvent contenir à la fois des bytes array et des chaines \n",
    "        # de caractères qui ne sont pas décodable, il faut juste extraire chaque chaine\n",
    "        # decodable et les concerver ainsi que leurs positions dans le texte original\n",
    "        # pour les rajouter apres.\n",
    "        \n",
    "        start_pos = encoded_text.find(\"\\\\\")\n",
    "        if start_pos != -1:\n",
    "            extractedstrings = []\n",
    "            bytes_stack = []\n",
    "            idx = 0\n",
    "\n",
    "            if start_pos > 0:\n",
    "                extractedstrings.append(ExtractedString(encoded_text[:start_pos], 0))\n",
    "                idx = 1\n",
    "\n",
    "            for byte_part in encoded_text[start_pos:].split(\"\\\\x\"):\n",
    "                try:\n",
    "                    _ = int(byte_part, 16)\n",
    "                    if len(byte_part) == 2:\n",
    "                        bytes_stack.append(byte_part)\n",
    "                    else:\n",
    "                        raise Exception\n",
    "                except:\n",
    "                    flag = byte_part\n",
    "                    try:\n",
    "                        _ = int(byte_part[:2], 16)\n",
    "                        bytes_stack.append(byte_part[:2])\n",
    "                        flag = byte_part[2:]\n",
    "                    except:\n",
    "                        pass\n",
    "                    extractedstrings.append(ExtractedString(flag, len(bytes_stack)//2 + idx))\n",
    "            bytes_string = \" \".join(bytes_stack)\n",
    "\n",
    "            decoded_text_array = list(bytearray.fromhex(bytes_string).decode())\n",
    "\n",
    "            #Replace extractedString in the decoded string\n",
    "            counter = 0\n",
    "            for extractedstring in extractedstrings:\n",
    "                decoded_text_array.insert(extractedstring.index + counter, extractedstring.string)\n",
    "                counter += 1\n",
    "                \n",
    "            #Merge the string in decoded array\n",
    "            return \"\".join(decoded_text_array)\n",
    "        return encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ouverture du fichier de lang et construction de la table de mappage \n",
    "# des articles aux langues dans lesquels ils sont traduit\n",
    "counter = 1\n",
    "key = 0\n",
    "\n",
    "\n",
    "lang_file_name = \"langlinks_encoded_version.csv\"\n",
    "\n",
    "lang_file = open(os.path.join(INPUT_DATA_FOLDER, lang_file_name), \"rt\", encoding=\"utf-8\")\n",
    "\n",
    "threads_alive = len(threading.enumerate())\n",
    "print(\"openning lang_file\")\n",
    "print(\"Successfully openend\")\n",
    "print(\"Looping on entries\")\n",
    "for line in lang_file:\n",
    "    list_strings[key].append(line)\n",
    "    if counter % MAX_BATCH == 0:\n",
    "        while len(threading.enumerate()) >= MAX_THREAD_START_NUMBER - 1:\n",
    "            flag = 1\n",
    "        th = threading.Thread(target=make_lang_mapper, args=(key,))\n",
    "        th.daemon = True\n",
    "        th.start()\n",
    "        key += 1\n",
    "    counter += 1\n",
    "else:\n",
    "    while len(threading.enumerate()) >= MAX_THREAD_START_NUMBER - 1:\n",
    "        flag = 1\n",
    "    th = threading.Thread(target=make_lang_mapper, args=(key,))\n",
    "    th.daemon = True\n",
    "    th.start()\n",
    "\n",
    "print('wait ends of thread')\n",
    "while len(threading.enumerate()) >= threads_alive:\n",
    "    print(len(threading.enumerate()))\n",
    "    flag = 1\n",
    "lang_file.close()\n",
    "print(\"Pages langs dictionary is successfull builded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ouverture du fichier contenant les articles et extraction de ceux qui n'ont\n",
    "# ni de traduction en france; ni de traduction en francais et en anglais\n",
    "\n",
    "\n",
    "print(\"openning page file\")\n",
    "\n",
    "threads_alive = len(threading.enumerate())\n",
    "\n",
    "page_file_name = \"pages_encoded_version.csv\"\n",
    "output_without_fr_name = \"database_without_articles_translate_in_fr.csv\"\n",
    "output_without_fr_en_name = \"database_without_articles_translate_in_fr_and_en.csv\"\n",
    "\n",
    "key = 0\n",
    "counter = 1\n",
    "\n",
    "page_file = open(os.path.join(INPUT_DATA_FOLDER, page_file_name), \"rt\", encoding=\"utf-8\")\n",
    "output_without_fr_file = open(os.path.join(OUTPUT_DATA_FOLDER, output_without_fr_name), \"wt\", encoding=\"utf-8\")\n",
    "output_without_fr_en_file = open(os.path.join(OUTPUT_DATA_FOLDER,output_without_fr_en_name), \"wt\", encoding=\"utf-8\")\n",
    "    \n",
    "\n",
    "print(\"Successfully openned\")\n",
    "\n",
    "print(\"page_ids\\t\\tpage_titles\\t\\turl\", file=output_without_fr_file)\n",
    "print(\"page_ids\\t\\tpage_titles\\t\\turl\", file=output_without_fr_en_file)\n",
    "\n",
    "print(\"Looping on entries and insert data in ouput file\")\n",
    "for line in page_file:\n",
    "    pages_descriptions[key].append(line)\n",
    "    if counter % MAX_BATCH == 0:\n",
    "        while len(threading.enumerate()) >= MAX_THREAD_START_NUMBER - 1:\n",
    "            flag = 1\n",
    "        th = threading.Thread(target=find_right_fit_and_export, args=(key, output_without_fr_en_file, output_without_fr_file))\n",
    "        th.daemon = True\n",
    "        th.start()\n",
    "        key += 1\n",
    "    counter += 1\n",
    "else:\n",
    "    while len(threading.enumerate()) >= MAX_THREAD_START_NUMBER - 1:\n",
    "        flag = 1\n",
    "    th = threading.Thread(target=find_right_fit_and_export, args=(key, output_without_fr_en_file, output_without_fr_file))\n",
    "    th.daemon = True\n",
    "    th.start()\n",
    "\n",
    "print('wait ends of thread')\n",
    "while len(threading.enumerate()) > threads_alive:\n",
    "    print(len(threading.enumerate()))\n",
    "    flag = 1\n",
    "    \n",
    "page_file.close()\n",
    "output_without_fr_file.close()\n",
    "output_without_fr_en_file.close()\n",
    "\n",
    "print(\"Merging is end\")\n",
    "print(\"End of Processing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b6f5402f002e7eaef598d517c5fc7b68a4e1f60d710cb0cb09622b42f3890d09"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
